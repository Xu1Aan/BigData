# 电商数仓（用户行为数据采集）

## 1 数据仓库概念

数据仓库（ Data Warehouse ），是为企业所有决策制定过程，提供所有系统数据支持的战略集合。

通过对数据仓库中数据的分析，可以帮助企业，改进业务流程、控制成本、提高产品质量等。

数据仓库，并不是数据的最终目的地，而是为数据最终的目的地做好准备。这些准备包括对数据的：清洗，转义，分类，重组，合并，拆分，统计等等。

<img src=".\picture\数据仓库概念.png" style="zoom:22%;" />

## 2 项目需求及架构设计

### 2.1 项目需求分析

一、项目需求

1、用户行为数据采集平台搭建

2、业务数据采集平台搭建

3、数据仓库维度建模

4、分析，设备、会员、商品、地区、活动等电商核心主题，统计的报表指标近100个。完全对比中型公司。

5、采用即席查询工具，随时进行指标分析

6、对集群性能进行监控，发生异常需要报警。

7、元数据管理

8、质量监控

二、思考题

1、项目技术如何选型？

2、框架版本如何选型（Apache、CDH、HDP）

3、服务器使用物理机还是云主机？

4、如何确认集群规模？（假设每台服务器8T硬盘）

### 2.2 项目框架

#### 2.2.1 技术选型

技术选型主要考虑因素：数据量大小、业务需求、行业内经验、技术成熟度、开发维护成本、总成本预算

数据采集传输：Flume，Kafka，Sqoop ，Logstash，DataX，

数据存储：MySql，HDFS，HBase，Redis，MongoDB

数据计算：Hive，Tez， Spark， Flink，Storm

数据查询：Presto，Kylin ，Impala，Druid

数据可视化：Echarts、Superset、QuickBI、DataV

任务调度：Azkaban、Oozie

集群监控：Zabbix

元数据管理：Atlas

#### 2.2.2 系统数据流程设计

<img src="E:\learning\04_java\01_笔记\BigData\10_porject\01_数仓采集\picture\系统数据流程设计.png" style="zoom:22%;" />

#### 2.2.3 框架版本选型

1）如何选择Apache/CDH/HDP版本？

（1）Apache：运维麻烦，组件间兼容性需要自己调研。（一般大厂使用，技术实力雄厚，有专业的运维人员） **（**建议使用**）**

（2）CDH：国内使用最多的版本，但CM不开源，今年开始要收费，一个节点1万美金。

（3）HDP：开源，可以进行二次开发，但是没有CDH稳定，国内使用较少

（1）Apache框架版本

| **产品**      | **版本** |
| ------------- | -------- |
| **Hadoop**    | 3.1.3    |
| **Flume**     | 1.9.0    |
| **Kafka**     | 2.4.1    |
| **Hive**      | 3.1.2    |
| **Sqoop**     | 1.4.6    |
| **Java**      | 1.8      |
| **Zookeeper** | 3.5.7    |
| **Presto**    | 0.189    |

注意事项：框架选型尽量不要选择最新的框架，选择最新框架半年前左右的稳定版。

#### 2.2.4  服务器选型

**1）物理机：**

- 以128G内存，20核物理CPU，40线程，8THDD和2TSSD硬盘，戴尔品牌单台报价4W出头。一般物理机寿命5年左右。

- 需要有专业的运维人员，平均一个月1万。电费也是不少的开销。

**2）云主机：**

- 云主机：以阿里云为例，差不多相同配置，每年5W。

- 很多运维工作都由阿里云完成，运维相对较轻松

**3）企业选择**

- 金融有钱公司和阿里没有直接冲突的公司选择阿里云

- 中小公司、为了融资上市，选择阿里云，拉倒融资后买物理机。

- 有长期打算，资金比较足，选择物理机。

#### 2.2.5 集群资源规划设计

1）如何确认集群规模？（假设：每台服务器8T磁盘，128G内存）

（1）每天日活跃用户100万，每人一天平均100条：100万*100条=1亿条

（2）每条日志1K左右，每天1亿条：100000000 / 1024 / 1024 = 约100G

（3）半年内不扩容服务器来算：100G*180天=约18T

（4）保存3副本：18T*3=54T

（5）预留20%~30%Buf=54T/0.7=77T

（6）算到这：约8T*10台服务器

2）如果考虑数仓分层？数据采用压缩？需要重新再计算

3）测试集群服务器规划

| 服务名称              | 子服务           | 服务器  hadoop102 | 服务器  hadoop103 | 服务器  hadoop104 |
| --------------------- | ---------------- | ----------------- | ----------------- | ----------------- |
| HDFS                  | NameNode         | √                 |                   |                   |
| DataNode              | √                | √                 | √                 |                   |
| SecondaryNameNode     |                  |                   | √                 |                   |
| Yarn                  | NodeManager      | √                 | √                 | √                 |
| Resourcemanager       |                  | √                 |                   |                   |
| Zookeeper             | Zookeeper Server | √                 | √                 | √                 |
| Flume(采集日志)       | Flume            | √                 | √                 |                   |
| Kafka                 | Kafka            | √                 | √                 | √                 |
| Flume（消费Kafka）    | Flume            |                   |                   | √                 |
| Hive                  | Hive             | √                 |                   |                   |
| MySQL                 | MySQL            | √                 |                   |                   |
| Sqoop                 | Sqoop            | √                 |                   |                   |
| Presto                | Coordinator      | √                 |                   |                   |
| Worker                |                  | √                 | √                 |                   |
| Azkaban               | AzkabanWebServer | √                 |                   |                   |
| AzkabanExecutorServer | √                |                   |                   |                   |
| Druid                 | Druid            | √                 | √                 | √                 |
| Kylin                 |                  | √                 |                   |                   |
| Hbase                 | HMaster          | √                 |                   |                   |
| HRegionServer         | √                | √                 | √                 |                   |
| Superset              |                  | √                 |                   |                   |
| Atlas                 |                  | √                 |                   |                   |
| Solr                  | Jar              | √                 |                   |                   |
| 服务数总计            |                  | 18                | 9                 | 9                 |

---

## 3 数据生成模块

### 3.1 目标数据

我们要收集和分析的数据主要包括**页面数据**、**事件数据、曝光数据、启动数据和错误数据。**

#### 3.1.1 页面

页面数据主要记录一个页面的用户访问情况，包括访问时间、停留时间、页面路径等信息。

<img src=".\picture\页面数据.png" style="zoom:25%;" />

1）所有页面id如下

```text
home("首页"),
category("分类页"),
discovery("发现页"),
top_n("热门排行"),
favor("收藏页"),
search("搜索页"),
good_list("商品列表页"),
good_detail("商品详情"),
good_spec("商品规格"),
comment("评价"),
comment_done("评价完成"),
comment_list("评价列表"),
cart("购物车"),
trade("下单结算"),
payment("支付页面"),
payment_done("支付完成"),
orders_all("全部订单"),
orders_unpaid("订单待支付"),
orders_undelivered("订单待发货"),
orders_unreceipted("订单待收货"),
orders_wait_comment("订单待评价"),
mine("我的"),
activity("活动"),
login("登录"),
register("注册");
```

2）所有页面对象类型如下：

```text
sku_id("商品skuId"),
keyword("搜索关键词"),
sku_ids("多个商品skuId"),
activity_id("活动id"),
coupon_id("购物券id");
```

3）所有来源类型如下：

```text
promotion("商品推广"),
recommend("算法推荐商品"),
query("查询结果商品"),
activity("促销活动");
```

#### 3.1.2 事件

事件数据主要记录应用内一个具体操作行为，包括操作类型、操作对象、操作对象描述等信息。

<img src=".\picture\事件.png" style="zoom:33%;" />

1）所有动作类型如下：

```
favor_add("添加收藏"),
favor_canel("取消收藏"),
cart_add("添加购物车"),
cart_remove("删除购物车"),
cart_add_num("增加购物车商品数量"),
cart_minus_num("减少购物车商品数量"),
trade_add_address("增加收货地址"),
get_coupon("领取优惠券");
```

注：对于下单、支付等业务数据，可从业务数据库获取。

2）所有动作目标类型如下：

```
sku_id("商品"),
coupon_id("购物券");
```

#### 3.1.3 曝光

曝光数据主要记录页面所曝光的内容，包括曝光对象，曝光类型等信息。

![](E:\learning\04_java\01_笔记\BigData\10_porject\01_数仓采集\picture\曝光.png)

1）所有曝光类型如下：

```
promotion("商品推广"),
recommend("算法推荐商品"),
query("查询结果商品"),
activity("促销活动");
```

2）所有曝光对象类型如下：

```
sku_id("商品skuId"),
activity_id("活动id");
```

#### 3.1.4 启动

启动数据记录应用的启动信息。

1）所有启动入口类型如下：

```
icon("图标"),
notification("通知"),
install("安装后启动");
```

#### 3.1.5 错误

错误数据记录应用使用过程中的错误信息，包括错误编号及错误信息。

### 3.2 数据埋点

#### 3.2.1 主流埋点方式（了解）

目前主流的埋点方式，有代码埋点（前端/后端）、可视化埋点、全埋点三种。

**代码埋点**是通过调用埋点SDK函数，在需要埋点的业务逻辑功能位置调用接口，上报埋点数据。例如，我们对页面中的某个按钮埋点后，当这个按钮被点击时，可以在这个按钮对应的 OnClick 函数里面调用SDK提供的数据发送接口，来发送数据。

**可视化埋点**只需要研发人员集成采集 SDK，不需要写埋点代码，业务人员就可以通过访问分析平台的“圈选”功能，来“圈”出需要对用户行为进行捕捉的控件，并对该事件进行命名。圈选完毕后，这些配置会同步到各个用户的终端上，由采集 SDK 按照圈选的配置自动进行用户行为数据的采集和发送。

**全埋点**是通过在产品中嵌入SDK，前端自动采集页面上的全部用户行为事件，上报埋点数据，相当于做了一个统一的埋点。然后再通过界面配置哪些数据需要在系统里面进行分析。

#### 3.2.2 埋点数据日志结构

我们的日志结构大致可分为两类，一是普通页面埋点日志，二是启动日志。

普通页面日志结构如下，每条日志包含了，当前页面的页面信息，所有事件（动作）、所有曝光信息以及错误信息。除此之外，还包含了一系列公共信息，包括设备信息，地理位置，应用信息等，即下边的common字段。

1）普通页面埋点日志格式

```
{
  "common": {                  -- 公共信息
    "ar": "230000",              -- 地区编码
    "ba": "iPhone",              -- 手机品牌
    "ch": "Appstore",            -- 渠道
    "is_new": "1",--是否首日使用，首次使用的当日，该字段值为1，过了24:00，该字段置为0。
	"md": "iPhone 8",            -- 手机型号
    "mid": "YXfhjAYH6As2z9Iq", -- 设备id
    "os": "iOS 13.2.9",          -- 操作系统
    "uid": "485",                 -- 会员id
    "vc": "v2.1.134"             -- app版本号
  },
"actions": [                     --动作(事件)  
    {
      "action_id": "favor_add",   --动作id
      "item": "3",                   --目标id
      "item_type": "sku_id",       --目标类型
      "ts": 1585744376605           --动作时间戳
    }
  ],
  "displays": [
    {
      "displayType": "query",        -- 曝光类型
      "item": "3",                     -- 曝光对象id
      "item_type": "sku_id",         -- 曝光对象类型
      "order": 1,                      --出现顺序
      "pos_id": 2                      --曝光位置
    },
    {
      "displayType": "promotion",
      "item": "6",
      "item_type": "sku_id",
      "order": 2, 
      "pos_id": 1
    },
    {
      "displayType": "promotion",
      "item": "9",
      "item_type": "sku_id",
      "order": 3, 
      "pos_id": 3
    },
    {
      "displayType": "recommend",
      "item": "6",
      "item_type": "sku_id",
      "order": 4, 
      "pos_id": 2
    },
    {
      "displayType": "query ",
      "item": "6",
      "item_type": "sku_id",
      "order": 5, 
      "pos_id": 1
    }
  ],
  "page": {                       --页面信息
    "during_time": 7648,        -- 持续时间毫秒
    "item": "3",                  -- 目标id
    "item_type": "sku_id",      -- 目标类型
    "last_page_id": "login",    -- 上页类型
    "page_id": "good_detail",   -- 页面ID
    "sourceType": "promotion"   -- 来源类型
  },
"err":{                     --错误
"error_code": "1234",      --错误码
    "msg": "***********"       --错误信息
},
  "ts": 1585744374423  --跳入时间戳
}
```

​    2）启动日志格式

启动日志结构相对简单，主要包含公共信息，启动信息和错误信息。

```
{
  "common": {
    "ar": "370000",
    "ba": "Honor",
    "ch": "wandoujia",
    "is_new": "1",
    "md": "Honor 20s",
    "mid": "eQF5boERMJFOujcp",
    "os": "Android 11.0",
    "uid": "76",
    "vc": "v2.1.134"
  },
  "start": {   
    "entry": "icon",         --icon手机图标  notice 通知   install 安装后启动
    "loading_time": 18803,  --启动加载时间
    "open_ad_id": 7,        --广告页ID
    "open_ad_ms": 3449,    -- 广告总共播放时间
    "open_ad_skip_ms": 1989   --  用户跳过广告时点
  },
"err":{                     --错误
"error_code": "1234",      --错误码
    "msg": "***********"       --错误信息
},
  "ts": 1585744304000
}
```

#### 3.2.3 埋点数据上报时机

埋点数据上报时机包括两种方式。

方式一，在离开该页面时，上传在这个页面产生的所有数据（页面、事件、曝光、错误等）。优点，批处理，减少了服务器接收数据压力。缺点，不是特别及时。

方式二，每个事件、动作、错误等，产生后，立即发送。优点，响应及时。缺点，对服务器接收数据压力比较大。

### 3.3 服务器和JDK准备

#### 3.3.1 服务器准备

安装如下文档配置步骤，分别安装hadoop102、hadoop103、hadoop104三台主机。

#### 3.3.2 阿里云服务器准备（可选）

#### 3.3.3 JDK准备

**1）卸载现有JDK（3台节点）**

```
[xu1an@hadoop102 opt]# sudo rpm -qa | grep -i java | xargs -n1 sudo rpm -e --nodeps

[xu1an@hadoop103 opt]# sudo rpm -qa | grep -i java | xargs -n1 sudo rpm -e --nodeps

[xu1an@hadoop104 opt]# sudo rpm -qa | grep -i java | xargs -n1 sudo rpm -e --nodeps
```

**2）用SecureCRT工具将JDK导入到hadoop102的/opt/software文件夹下面**

**3）在Linux系统下的opt目录中查看软件包是否导入成功**

```
[xu1an@hadoop102 software]# ls /opt/software/
```

**4）解压JDK到/opt/module目录下**

```
[xu1an@hadoop102 software]# tar -zxvf jdk-8u212-linux-x64.tar.gz -C /opt/module/
```

**7）配置JDK环境变量**

​    （1）新建/etc/profile.d/my_env.sh文件

```
[xu1an@hadoop102 module]# sudo vim /etc/profile.d/my_env.sh
```

添加如下内容，然后保存（:wq）退出

```
#JAVA_HOME
export JAVA_HOME=/opt/module/jdk1.8.0_212
export PATH=$PATH:$JAVA_HOME/bin
```

​    （2）让环境变量生效

```
[xu1an@hadoop102 software]$ source /etc/profile.d/my_env.sh
```

**8）测试JDK是否安装成功**

```
[xu1an@hadoop102 module]# java -version
```

如果能看到以下结果、则Java正常安装

```
java version "1.8.0_212"
```

**9）分发JDK** 

```
[xu1an@hadoop102 module]$ xsync /opt/module/jdk1.8.0_212/
```

**10）分发环境变量配置文件**

```
[xu1an@hadoop102 module]$ sudo /home/xu1an/bin/xsync /etc/profile.d/my_env.sh
```

**11）分别在hadoop103、hadoop104上执行source**

```
[xu1an@hadoop103 module]$ source /etc/profile.d/my_env.sh

[xu1an@hadoop104 module]$ source /etc/profile.d/my_env.sh
```

#### 3.3.4 环境变量配置说明

Linux的环境变量可在多个文件中配置，如/etc/profile，/etc/profile.d/*.sh，~/.bashrc，~/.bash_profile等，下面说明上述几个文件之间的关系和区别。

bash的运行模式可分为login shell和non-login shell。

例如，我们通过终端，输入用户名、密码，登录系统之后，得到就是一个login shell，而当我们执行以下命令ssh hadoop103 command，在hadoop103执行command的就是一个non-login shell。

这两种shell的主要区别在于，它们启动时会加载不同的配置文件，login shell启动时会加载/etc/profile，~/.bash_profile，~/.bashrc，non-login shell启动时会加载~/.bashrc。

而在加载~/.bashrc（实际是~/.bashrc中加载的/etc/bashrc）或/etc/profile时，都会执行如下代码片段，

![](.\picture\环境变量配置说明.png)

因此不管是login shell还是non-login shell，启动时都会加载/etc/profile.d/*.sh中的环境变量。

### 3.4 模拟数据

### 3.4.1 使用说明

**1）将application.ym、gmall2020-mock-log-2021-01-22.jar、path.json、logback.xml上传到hadoop102的/opt/module/applog目录下**

**（1）创建applog路径**

```
[xu1an@hadoop102 module]$ mkdir /opt/module/applog
```

**（2）上传文件**

**2）配置文件**

（1）application.yml文件

可以根据需求生成对应日期的用户行为日志。

```
[xu1an@hadoop102 applog]$ vim application.yml
```

修改如下内容

```
# 外部配置打开
# 外部配置打开
logging.config: "./logback.xml"
#业务日期
mock.date: "2020-06-14"

#模拟数据发送模式
#mock.type: "http"
#mock.type: "kafka"
mock.type: "log"

#http模式下，发送的地址
mock.url: "http://hdp1/applog"

#kafka模式下，发送的地址
mock:
  kafka-server: "hdp1:9092,hdp2:9092,hdp3:9092"
  kafka-topic: "ODS_BASE_LOG"

#启动次数
mock.startup.count: 200
#设备最大值
mock.max.mid: 500000
#会员最大值
mock.max.uid: 100
#商品最大值
mock.max.sku-id: 35
#页面平均访问时间
mock.page.during-time-ms: 20000
#错误概率 百分比
mock.error.rate: 3
#每条日志发送延迟 ms
mock.log.sleep: 10
#商品详情来源  用户查询，商品推广，智能推荐, 促销活动
mock.detail.source-type-rate: "40:25:15:20"
#领取购物券概率
mock.if_get_coupon_rate: 75
#购物券最大id
mock.max.coupon-id: 3
#搜索关键词  
mock.search.keyword: "图书,小米,iphone11,电视,口红,ps5,苹果手机,小米盒子"
```

（2）path.json，该文件用来配置访问路径

根据需求，可以灵活配置用户点击路径。

```
[
  {"path":["home","good_list","good_detail","cart","trade","payment"],"rate":20 },
  {"path":["home","search","good_list","good_detail","login","good_detail","cart","trade","payment"],"rate":40 },
  {"path":["home","mine","orders_unpaid","trade","payment"],"rate":10 },
  {"path":["home","mine","orders_unpaid","good_detail","good_spec","comment","trade","payment"],"rate":5 },
  {"path":["home","mine","orders_unpaid","good_detail","good_spec","comment","home"],"rate":5 },
  {"path":["home","good_detail"],"rate":10 },
  {"path":["home"  ],"rate":10 }
]
```

（3）logback配置文件

可配置日志生成路径，修改内容如下

```
<?xml version="1.0" encoding="UTF-8"?>
<configuration>
    <property name="LOG_HOME" value="/opt/module/applog/log" />
    <appender name="console" class="ch.qos.logback.core.ConsoleAppender">
        <encoder>
            <pattern>%msg%n</pattern>
        </encoder>
    </appender>

    <appender name="rollingFile" class="ch.qos.logback.core.rolling.RollingFileAppender">
        <rollingPolicy class="ch.qos.logback.core.rolling.TimeBasedRollingPolicy">
            <fileNamePattern>${LOG_HOME}/app.%d{yyyy-MM-dd}.log</fileNamePattern>
        </rollingPolicy>
        <encoder>
            <pattern>%msg%n</pattern>
        </encoder>
    </appender>

    <!-- 将某一个包下日志单独打印日志 -->
    <logger name="com.atgugu.gmall2020.mock.log.util.LogUtil"
            level="INFO" additivity="false">
        <appender-ref ref="rollingFile" />
        <appender-ref ref="console" />
    </logger>

    <root level="error"  >
        <appender-ref ref="console" />
    </root>
</configuration>
```

**3）生成日志**

（1）进入到/opt/module/applog路径，执行以下命令

```
[xu1an@hadoop102 applog]$ java -jar gmall2020-mock-log-2021-01-22.jar
```

（2）在/opt/module/applog/log目录下查看生成日志

```
[xu1an@hadoop102 log]$ ll
```

#### 3.4.2 集群日志生成脚本

**在hadoop102的/home/xu1an目录下创建bin目录，这样脚本可以在服务器的任何目录执行。**

```
[xu1an@hadoop102 ~]$ echo $PATH
/usr/local/bin:/usr/bin:/usr/local/sbin:/usr/sbin:/home/xu1an/.local/bin:/home/xu1an/bin
```

**1）在/home/atguigu/bin目录下创建脚本lg.sh**

```
[atguigu@hadoop102 bin]$ vim lg.sh
```

**2）在脚本中编写如下内容**

```
#!/bin/bash
for i in hadoop102 hadoop103; do
    echo "========== $i =========="
    ssh $i "cd /opt/module/applog/; java -jar gmall2020-mock-log-2021-01-22.jar >/dev/null 2>&1 &"
done 
```

注：

（1）/opt/module/applog/为jar包及配置文件所在路径

（2）/dev/null代表linux的空设备文件，所有往这个文件里面写入的内容都会丢失，俗称“黑洞”。

标准输入0：从键盘获得输入 /proc/self/fd/0 

标准输出1：输出到屏幕（即控制台） /proc/self/fd/1 

错误输出2：输出到屏幕（即控制台） /proc/self/fd/2

**3）修改脚本执行权限**

```
[atguigu@hadoop102 bin]$ chmod u+x lg.sh
```

**4）将jar包及配置文件上传至hadoop103的/opt/module/applog/**路径**

**5）启动脚本**

```
[atguigu@hadoop102 module]$ lg.sh 
```

**6）分别在hadoop102、hadoop103的/opt/module/applog/log目录上查看生成的数据**

```
[atguigu@hadoop102 logs]$ ls
app.2020-06-14.log
[atguigu@hadoop103 logs]$ ls
app.2020-06-14.log
```

---

## 4 数据采集模块

### 4.1 集群所有进程查看脚本

1）在/home/atguigu/bin目录下创建脚本xcall.sh

```
[atguigu@hadoop102 bin]$ vim xcall.sh
```

 2）在脚本中编写如下内容

```
#! /bin/bash
 
for i in hadoop102 hadoop103 hadoop104
do
    echo --------- $i ----------
    ssh $i "$*"
done
```

3）修改脚本执行权限

```
[atguigu@hadoop102 bin]$ chmod 777 xcall.sh
```

4）启动脚本

```
[atguigu@hadoop102 bin]$ xcall.sh jps
```

### 4.2 Hadoop安装

详见：Hadoop集群搭建

1）集群规划：

|      | 服务器hadoop102    |       服务器hadoop103        | 服务器hadoop104             |
| ---- | ------------------ | :--------------------------: | --------------------------- |
| HDFS | NameNode  DataNode |           DataNode           | DataNode  SecondaryNameNode |
| Yarn | NodeManager        | Resourcemanager  NodeManager | NodeManager                 |
|      |                    |         NodeManager          |                             |

